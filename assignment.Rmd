---
title: 'Peer-graded Assignment: Prediction Assignment Writeup'
author: "Miguel Santana"
date: "22/03/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Synopsis

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement -- a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, the goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: <http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har>.

This is document related to the submission for Peer-graded Assignment for Practical Machine Learning from John Hopkins University at Coursera. It was done using RStudio and RMarkdown files. All of the libraries are imported below.

## Loading and cleaning the data

```{r}
set.seed(12345)
library(lattice)
library(ggplot2)
library(caret)
train <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
train_data <- read.csv("pml-training.csv", na.strings = c("", "NA"))
submit <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
test_data <- read.csv("pml-testing.csv", na.strings = c("", "NA"))
```

Divide the original training dataset into two partitions, with one partition consisting of 75% of the data and the other partition consisting of 25%.

```{r}
in_train  <- createDataPartition(train_data$classe, p=0.75, list=FALSE)
train_set <- train_data[ in_train, ]
test_set  <- train_data[-in_train, ]
```

Both the training dataset (`train_set`) and the test dataset (`test_set`) contain a significant number of `NA` values and near-zero-variance (NZV) variables, which will be eliminated along with their corresponding ID variables.

```{r}
nzv_var <- nearZeroVar(train_set)
train_set <- train_set[ , -nzv_var]
test_set  <- test_set [ , -nzv_var]
```

Eliminate variables that contain mostly NA values, using a threshold of 95%.

```{r}
na_var <- sapply(train_set, function(x) mean(is.na(x))) > 0.95
train_set <- train_set[ , na_var == FALSE]
test_set  <- test_set [ , na_var == FALSE]
```

As columns 1 to 5 serve only as identification variables, they will also be eliminated.

```{r}
train_set <- train_set[ , -(1:5)]
test_set  <- test_set [ , -(1:5)]
```

## Processing Data

### Correlation Analysis

Perform a correlation analysis on the variables before beginning the modeling process, using the first principal component order (FPC).

```{r}
library(corrplot)
corr_matrix <- cor(train_set[ , -54])
corrplot(corr_matrix, order = "FPC", method = "circle", type = "lower",
         tl.cex = 0.6, tl.col = rgb(0, 0, 0))
```

When two variables are strongly correlated, they are represented by dark blue (for positive correlation) or dark red (for negative correlation) colors. However, since there are only a few significant correlations among the input variables, Principal Components Analysis (PCA) will not be conducted in this analysis. Instead, several prediction models will be developed to improve accuracy.

### Prediction Models - Decision Tree Model

Loading additional libraries:
```{r}
library(randomForest)
library(RColorBrewer)
```


```{r}
library(rpart)
library(rpart.plot)
library(rattle)
set.seed(2222)
fit_decision_tree <- rpart(classe ~ ., data = train_set, method="class")
fancyRpartPlot(fit_decision_tree)
```

Application of the decision tree model to the `test_set` for making predictions.

```{r}
predict_decision_tree <- predict(fit_decision_tree, newdata = test_set, type="class")
conf_matrix_decision_tree <- confusionMatrix(predict_decision_tree, factor(test_set$classe))
conf_matrix_decision_tree
```

The decision tree model has a relatively low predictive accuracy of **72.82%**. Here is a plot for predictive accuracy:

```{r}
plot(conf_matrix_decision_tree$table, col = conf_matrix_decision_tree$byClass, 
     main = paste("Decision Tree Model: Predictive Accuracy =",
                  round(conf_matrix_decision_tree$overall['Accuracy'], 4)))
```

### Prediction Models - Generalized Boosted Model (GBM)

Before start using the `train` function from the `caret` package, we need to do some parallellization of the work that is does, so we can produce a faster output, according to https://github.com/lgreski/datasciencectacontent/blob/master/markdown/pml-randomForestPerformance.md.


```{r}
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
```

```{r}
set.seed(2222)
ctrl_GBM <- trainControl(method = "repeatedcv", number = 5, repeats = 2, allowParallel = TRUE)
fit_GBM  <- train(classe ~ ., data = train_set, method = "gbm",
                  trControl = ctrl_GBM, verbose = FALSE)
fit_GBM$finalModel
```

Predictions of the GBM on `test_set`.

```{r}
predict_GBM <- predict(fit_GBM, newdata = test_set)
conf_matrix_GBM <- confusionMatrix(predict_GBM, factor(test_set$classe))
conf_matrix_GBM
```

The predictive accuracy of the GBM is relatively high at **98.51%**.

### Prediction Models - Random Forest Model

```{r}
set.seed(2222)
ctrl_RF <- trainControl(method = "repeatedcv", number = 5, repeats = 2, allowParallel = TRUE)
fit_RF  <- train(classe ~ ., data = train_set, method = "rf",
                  trControl = ctrl_RF, verbose = FALSE)
fit_RF$finalModel
```

Predictions of the random forest model on `test_set`.

```{r}
predict_RF <- predict(fit_RF, newdata = test_set)
conf_matrix_RF <- confusionMatrix(predict_RF, factor(test_set$classe))
conf_matrix_RF
```

The predictive accuracy of the Random Forest model is excellent at **99.8%**.

## Application of the best predictive model to the test data

Below are the predictive accuracies of the three models:

- Decision Tree Model: 72.82%
- Generalized Boosted Model: 98.51%
- Random Forest Model: 99.80%

The Random Forest model is chosen and used to make predictions for the 20 data points in the original testing dataset (data_quiz).

```{r}
predict_data <- as.data.frame(predict(fit_RF, newdata = test_data))
predict_data
```

```{r include=FALSE}
stopCluster(cluster)
registerDoSEQ()
```

